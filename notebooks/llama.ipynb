{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_DATASETS_CACHE=\"/scratch/pt2295/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/pt2295/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"/scratch/pt2295/LlvmProject/automatic_prompt_engineer/data/bigbench-ii/epistemic_reasoning/task.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    \n",
    "    data_orig = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_orig['examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_orig['examples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "\n",
    "# Define the proportions for train, test, and validation sets\n",
    "total_samples = len(data)\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "validation_ratio = 0.15\n",
    "\n",
    "# Calculate the sizes of each set\n",
    "train_size = int(total_samples * train_ratio)\n",
    "test_size = int(total_samples * test_ratio)\n",
    "validation_size = int(total_samples * validation_ratio)\n",
    "\n",
    "# Split the data into sets\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:train_size + test_size]\n",
    "validation_data = data[train_size + test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=train_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Premise: Charlotte suspects that a man in gray on a rocky cliff, overlooking the mountains. Hypothesis: Charlotte suspects that a mountain climber has reached the top of a cliff.',\n",
       " 'target_scores': {'entailment': 1, 'non-entailment': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description=data_orig['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type='entailment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix=data_orig['task_prefix']\n",
    "if task_type=='hyperbaton':\n",
    "    task_prefix=task_prefix.strip()\n",
    "    task_prefix+=\" Choose only from the following options: 'a' or 'b'.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(length,line,task_type='entailment'):\n",
    "    line=line[length:]\n",
    "    pattern = r'<Ans>(.*?)</Ans>|Ans:\\s*([\\w-]+)'\n",
    "    matches = re.findall(pattern, line)\n",
    "    if len(matches)==0:#right now if not match return\n",
    "        return -1\n",
    "    if task_type=='entailment':\n",
    "        if matches =='entailment' or matches=='Entailment' or \"non-entail\" not in matches:\n",
    "            return 1\n",
    "    elif task_type=='hyperbaton':\n",
    "        if matches[0][0]=='a' or matches[0][0]=='A':\n",
    "            return 1\n",
    "    return 0\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# custom_cache_dir = \"/scratch/pt2295/cache/\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=custom_cache_dir,use_fast=True)\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True,cache_dir=custom_cache_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def get_api(inp):\n",
    "    def query(payload):\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        return response.json()\n",
    "    \n",
    "    output = query({\n",
    "        \"inputs\": inp,\n",
    "#         \"parameters\":{\"max_new_tokens\":1024}\n",
    "    })\n",
    "#     print(\"output is\",output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_llm(user_prompt,use_api=False):\n",
    "# user_prompt=\"Determine whether one sentence entails the next. Given input: Premise: William suspects that Isabella learns that two men are in blue life jackets and sitting on innertubes with 'Crusher' written on them in the middle of a body of water. Hypothesis: Isabella learns that two men are in blue life jackets and sitting on innertubes with 'Crusher' written on them in the middle of a body of water.Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'. Put your answer within tag <Ans> and </Ans>\"\n",
    "    sys_prompt=\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "    prompt=f\"<s>[INST] {user_prompt} [/INST]\"\n",
    "    \n",
    "    if use_api==False:\n",
    "   \n",
    "        model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        output = model.generate(**model_inputs)\n",
    "\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    else:\n",
    "#         print(get_api(prompt))\n",
    "#         print(get_api(prompt)[0])\n",
    "        try:\n",
    "            return get_api(prompt)[0]['generated_text']\n",
    "        except:\n",
    "            return \"\"\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "def run_inference_step1(prompt_t,use_api=False,task_type='entailment'):\n",
    "    wrong_ans_indices=[]\n",
    "    not_got=[]\n",
    "    temp_prompt_t=prompt_t\n",
    "    for i in range(len(train_data)):\n",
    "        \n",
    "        prompt_t=temp_prompt_t.replace('<INPUT>',train_data[i]['input'])\n",
    "        op=get_answer_llm(prompt_t,use_api)\n",
    "        if op=='':\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        output_str=op\n",
    "        \n",
    "#         print(output_str)\n",
    "        ans=extract_answer(len(prompt_t),output_str,task_type)\n",
    "        if ans==-1:\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        ground_truth=1\n",
    "      \n",
    "        if task_type=='entailment':\n",
    "            if train_data[i]['target_scores']['non-entailment']==1:\n",
    "                ground_truth=0\n",
    "        elif task_type=='hyperbaton':\n",
    "            if train_data[i]['target_scores']['b']==1:\n",
    "                ground_truth=0\n",
    "        if (ans!=ground_truth):\n",
    "            wrong_ans_indices.append(i)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    return wrong_ans_indices,not_got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output is [{'generated_text': '<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Michael suspects that Thomas sees that two kids in blue wetsuits surf. Hypothesis: Thomas sees that two kids in blue wetsuits surf.\\nIdentify the relation between the following premises and hypotheses, choosing from the options \\'entailment\\' or \\'non-entailment\\'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]  <Ans>Entailment</Ans>\\n\\nExplanation: The hypothesis \"Thomas sees that two kids in blue wetsuits surf\" is a logical consequence of the premise \"Michael suspects that Thomas sees that two kids in blue wetsuits surf\". The premise implies that Thomas has some level of awareness or knowledge about the activity of the two kids in blue wetsuits, and the hypothesis specifically states what that awareness or knowledge is. Therefore, the hypothesis entails the premise.'}]\n",
      "output is [{'generated_text': '<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Charles suspects that four children are wearing blue hats with colored streamers. Hypothesis: Charles suspects that four kids are wearing hats with streamers.\\nIdentify the relation between the following premises and hypotheses, choosing from the options \\'entailment\\' or \\'non-entailment\\'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]  <Ans>Entailment</Ans>\\n\\nExplanation:\\nThe hypothesis \"Charles suspects that four kids are wearing hats with streamers\" is a broader statement than the premise \"Charles suspects that four children are wearing blue hats with colored streamers.\" The premise implies that the hats are blue and have colored streamers, while the hypothesis only states that the hats have streamers. Therefore, the hypothesis entails the premise.'}]\n",
      "output is [{'generated_text': '<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Richard assumes that a person is sitting on a yellow picnic table with a water bottle, while a baby stroller sits beside her. Hypothesis: Taylor assumes that a individual on a table holds a water near a stroller.\\nIdentify the relation between the following premises and hypotheses, choosing from the options \\'entailment\\' or \\'non-entailment\\'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]  <Ans>Entailment</Ans>\\n\\nExplanation:\\nThe hypothesis \"Taylor assumes that an individual on a table holds a water near a stroller\" can be inferred from the premise \"Richard assumes that a person is sitting on a yellow picnic table with a water bottle, while a baby stroller sits beside her.\" The premise provides more specific information about the situation, including the color of the picnic table and the presence of a baby stroller, which are not necessary for the hypothesis. Therefore, the hypothesis logically follows from the premise, making it an entailment.'}]\n",
      "output is [{'generated_text': \"<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Robert suspects that Evelyn knows that a man wearing red ski pants, a black jacket, and a white helmet is skiing down a mountain. Hypothesis: Evelyn knows that a man wearing red ski pants, a black jacket, and a white helmet is skiing down a mountain.\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]  <Ans>Entailment</Ans>\\n\\nExplanation: The hypothesis is a logical consequence of the premise. The premise states that Robert suspects that Evelyn knows something, and the hypothesis states that Evelyn knows that same thing. Since the premise implies that Evelyn has knowledge of the man's appearance, it logically follows that Evelyn knows that the man is skiing down the mountain. Therefore, the premise entails the hypothesis.\"}]\n",
      "output is [{'generated_text': \"<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Amelia believes that Charlotte assumes that a cheerful boy rides a makeshift sled down a snowy hill. Hypothesis: Amelia believes that a cheerful boy rides a makeshift sled down a snowy hill.\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]  <Ans>Entailment</Ans>\\n\\nExplanation:\\nThe premise states that Amelia believes that Charlotte assumes a cheerful boy rides a makeshift sled down a snowy hill. The hypothesis states that Amelia believes a cheerful boy rides a makeshift sled down a snowy hill. Since the premise implies that Amelia has knowledge of Charlotte's assumption, and the hypothesis states Amelia's belief, which is in line with Charlotte's assumption, the hypothesis is entailed by the premise.\"}]\n",
      "output is [{'generated_text': \"<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Amelia understands that Thomas thinks that construction workers use cables to hang from the sides of buildings. Hypothesis: Thomas thinks that construction workers use cables to hang from the sides of buildings.\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]  <Ans>Entailment</Ans>\\n\\nExplanation: The hypothesis is a paraphrased version of the premise, so it logically follows from the premise. The premise states that Amelia understands that Thomas thinks that construction workers use cables to hang from the sides of buildings, and the hypothesis states that Thomas thinks that construction workers use cables to hang from the sides of buildings. Therefore, the hypothesis is entailed by the premise.\"}]\n",
      "output is [{'generated_text': '<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Olivia suspects that a gymnast is seen defying gravity as she performs an impressive move several feet above the parallel bars. Hypothesis: Olivia suspects that the gymnast is a woman.\\nIdentify the relation between the following premises and hypotheses, choosing from the options \\'entailment\\' or \\'non-entailment\\'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]  <Ans>Entailment</Ans>\\n\\nExplanation: The premise \"Olivia suspects that a gymnast is seen defying gravity as she performs an impressive move several feet above the parallel bars\" implies that the gymnast is performing a impressive move, and since the move is impressive, it is likely that the gymnast is a woman. Therefore, the hypothesis \"Olivia suspects that the gymnast is a woman\" is entailed by the premise.'}]\n",
      "output is [{'generated_text': '<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Joseph recognizes that John thinks that skateboarder sliding down a ramp. Hypothesis: John thinks that skateboarder sliding down a ramp.\\nIdentify the relation between the following premises and hypotheses, choosing from the options \\'entailment\\' or \\'non-entailment\\'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]  <Ans>Entailment</Ans>\\n\\nExplanation: The premise \"Joseph recognizes that John thinks that skateboarder sliding down a ramp\" implies that John has the thought or belief that the skateboarder is sliding down a ramp. Therefore, the hypothesis \"John thinks that skateboarder sliding down a ramp\" is entailed by the premise.'}]\n",
      "output is [{'generated_text': '<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Evelyn knows that James suspects that a man, wearing a white hate and sunglasses, is sitting in a white truck that says \"Coast Gas\". Hypothesis: James suspects that Evelyn knows that a man, wearing a white hate and sunglasses, is sitting in a white truck that says \"Coast Gas\".\\nIdentify the relation between the following premises and hypotheses, choosing from the options \\'entailment\\' or \\'non-entailment\\'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]  <Ans>Entailment</Ans>\\n\\nExplanation:\\nThe hypothesis states that James suspects that Evelyn knows something, which is exactly what the premise states. Therefore, the hypothesis logically follows from the premise, making it an entailment.'}]\n",
      "output is [{'generated_text': \"<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Abigail thinks that 3 people are shopping in a market. Hypothesis: David thinks that there are people at a market.\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]  <Ans>Entailment</Ans>\\n\\nExplanation:\\nThe premise states that Abigail thinks that 3 people are shopping in a market, which means that at least 3 people are present in the market. The hypothesis states that David thinks that there are people at a market, which means that David believes that the market is not empty. Since the premise implies that there are at least 3 people in the market, it logically follows that David's belief that there are people at the market is true. Therefore, the premise entails the hypothesis.\"}]\n"
     ]
    }
   ],
   "source": [
    "wrong_answers,not_got=run_inference_step1(prompt_t,True,task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer not only within tags, difficult to extract answer -> solved by improving prompt\n",
    "#70b is good but v slow\n",
    "#maybe use pipeline with datasets or use more gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hint(length,line):\n",
    "    line=line[length:]\n",
    "    pattern = r'<hint>(.*?)</hint>'\n",
    "    matches = re.findall(pattern, line)\n",
    "    if len(matches)==0:\n",
    "        return \"\"\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (wrong_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 9]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hints_residual_step2(wrong_answers,use_api=True):\n",
    "    hints_wrong_ans={}\n",
    "    for idx in wrong_answers:\n",
    "        if task_type=='entailment':\n",
    "            if train_data[idx]['target_scores']['entailment']==1:\n",
    "                ans='entailment'\n",
    "            else:\n",
    "                ans='non-entailment'\n",
    "        prompt_h='Given following task:'+task_description+'\\n'+'Given input: '+train_data[idx]['input']+'\\n'+'And its expected output: '+ans+'\\n'+'List the reason or hint why its with this expected output within tag <hint> and </hint>. The hint or explaination should be necassarily between the tags.'\n",
    "       \n",
    "        op=get_answer_llm(prompt_h,use_api)\n",
    "#         print(\"output hint is\",op)\n",
    "        hint=extract_hint(len(prompt_h),op)\n",
    "       \n",
    "        if hint!='':\n",
    "#             hints_wrong_ans.append((idx,hint))\n",
    "            hints_wrong_ans[idx]=hint\n",
    "        \n",
    "    return hints_wrong_ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "hints_wrong=get_hints_residual_step2(wrong_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: \"The premise states that Charlotte believes that Thomas sees something, but it doesn't necessarily mean that Thomas actually sees it. The hypothesis is stating what Thomas sees, which may or may not be the same as what Charlotte believes he sees. Therefore, the hypothesis does not logically follow from the premise, making it a non-entailment.\",\n",
       " 1: \"The premise mentions Evelyn's assumption about Robert's knowledge, but the hypothesis only states Robert's knowledge without mentioning Evelyn's assumption. Therefore, the hypothesis does not logically follow from the premise, and the relationship between the two sentences is not one of entailment.\",\n",
       " 9: \"The premise mentions Ava's recognition of Olivia's suspicion, but the hypothesis only mentions Olivia's suspicion of Ava's recognition. The two sentences do not perfectly align, and the hypothesis does not explicitly state that Ava recognizes the woman holding the check, which means the hypothesis does not logically follow from the premise. Therefore, the relationship between the two sentences is non-entailment.\"}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hints_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hints_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(wrong_answers,k=2):\n",
    "    selected=random.sample(wrong_answers, k)\n",
    "    return selected\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarise_step4(wrong_answers, hints_wrong,use_api=True,):\n",
    "    selected=random_sampling(list(hints_wrong.keys()))#HERE I HAVED USED HINTS WRONG INSTEAD OF WRONG_ANS \n",
    "    prompt_s=\"This is a task to \"+task_description+\". We have some expected input and output pairs and have asked labeler to give reason or hint for each expected output. Given following data each contains input, output and reason for the expected output, summarize a general reason for all these cases:\"+'\\n'\n",
    "    temp_str=''\n",
    "    ctr=1\n",
    "    for idx in selected:\n",
    "        temp=''\n",
    "        if task_type=='entailment':\n",
    "            if train_data[idx]['target_scores']['entailment']==1:\n",
    "                ans='entailment'\n",
    "            else:\n",
    "                ans='non-entailment'\n",
    "\n",
    "        temp+='Given input: '+train_data[idx]['input']+'\\n'+'And its expected output: '+ans+'. And the reason for the expected output: '+hints_wrong[idx]+'\\n'\n",
    "        temp_str+=temp+'\\n'\n",
    "        \n",
    "    prompt_s+=temp_str+'\\n'+\"Summarised reason output:\"\n",
    "#     print(prompt_s)\n",
    "    l=len(prompt_s)\n",
    "    op=get_answer_llm(prompt_s,use_api)#[l+8+7:]#or extract after [/INSTR]\n",
    "   \n",
    "    op=op[len(prompt_s)+9+10:]\n",
    "#     print(op)\n",
    "    return op.lstrip()\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarised_prompt=get_summarise_step4(wrong_answers,hints_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt=task_description+\"\\n\"+'Some useful hints are: '+summarised_prompt+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Determine whether one sentence entails the next\\nSome useful hints are: Based on the given input and output pairs, the general reason for non-entailment can be summarized as follows:\\n\\nThe premises and hypotheses are either identical, or the hypothesis does not provide any new information or reasoning beyond what is already stated in the premise. In some cases, the premise only provides information about the beliefs or assumptions of one character, while the hypothesis makes assumptions about the beliefs or knowledge of another character. Additionally, some premises and hypotheses are phrased as statements about beliefs or assumptions, rather than providing new information or reasoning. As a result, there is no entailment between the premises and hypotheses.\\nGiven input: <INPUT>\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have to replicate all these steps for T iterations, just a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_iterations_T=10\n",
    "succesive_prompts=[]\n",
    "start=time.time()\n",
    "prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "for t in range(num_iterations_T):\n",
    "    wrong_answers=run_inference_step1(prompt_t,task_type)\n",
    "#     print(len(wrong_answers))\n",
    "    hints_wrong=get_hints_residual_step2(wrong_answers)\n",
    "#     print(hints_wrong)\n",
    "    summarised_prompt=get_summarise_step4(wrong_answers,hints_wrong)\n",
    "#     print(summarised_prompt)\n",
    "    final_prompt=task_description+\"\\n\"+'Some useful hints are: '+summarised_prompt+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "    succesive_prompts.append(final_prompt)\n",
    "    prompt_t=final_prompt\n",
    "#     print(prompt_t)\n",
    "print(\"TIme taken\",time.time()-start)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succesive_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now try inference api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b-chat-hf\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "inp=\"Determine whether one sentence entails the next\\nSome useful hints are: Based on the given input and output pairs, the general reason for non-entailment is that the hypothesis does not provide sufficient information to logically follow from the premise. The premises provide specific details or context that are not present in the hypothesis, or the hypothesis does not provide any new information or insights beyond what is already implied in the premise. Additionally, some of the premises provide subjective interpretations or assumptions, which do not necessarily imply factual or objective truth. Therefore, the output for all the given input pairs is expected to be non-entailment.\\nGiven input: Premise: James assumes that David assumes that asian men sitting down, and some of them are wearing white and pink hats, while the others are wearing blue jackets with yellow sashes. Hypothesis: David assumes that asian men sitting down, and some of them are wearing white and pink hats, while the others are wearing blue jackets with yellow sashes. \\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between <Ans> and </Ans>.\"\n",
    "output = query({\n",
    "\t\"inputs\": inp,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Determine whether one sentence entails the next\\nSome useful hints are: Based on the given input and output pairs, the general reason for non-entailment is that the hypothesis does not provide sufficient information to logically follow from the premise. The premises provide specific details or context that are not present in the hypothesis, or the hypothesis does not provide any new information or insights beyond what is already implied in the premise. Additionally, some of the premises provide subjective interpretations or assumptions, which do not necessarily imply factual or objective truth. Therefore, the output for all the given input pairs is expected to be non-entailment.\\nGiven input: Premise: James assumes that David assumes that asian men sitting down, and some of them are wearing white and pink hats, while the others are wearing blue jackets with yellow sashes. Hypothesis: David assumes that asian men sitting down, and some of them are wearing white and pink hats, while the others are wearing blue jackets with yellow sashes. \\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between <Ans> and </Ans>.\\n\\n<Question>\\n<Ans>Entailment</Ans>\\n</Question\"}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=inp=\"<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Taylor assumes that Sophia believes that a man wearing a brown shirt and backpack is pushing a stroller while a small toddler is walking along aside him on a rain-covered street. Hypothesis: Taylor assumes that a man wearing a brown shirt and backpack is pushing a stroller while a small toddler is walking along aside him on a rain-covered street.\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<Question>\\n<Ans>Entailment</Ans>\\n</Question'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]['generated_text'][len(inp):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"Determine whether one sentence entails the next\\nSome useful hints are: Based on the given input and output pairs, the general reason for non-entailment is that the hypothesis does not provide sufficient information to logically follow from the premise. The premises provide specific details or context that are not present in the hypothesis, or the hypothesis does not provide any new information or insights beyond what is already implied in the premise. Additionally, some of the premises provide subjective interpretations or assumptions, which do not necessarily imply factual or objective truth. Therefore, the output for all the given input pairs is expected to be non-entailment.\\nGiven input: <INPUT>\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Determine whether one sentence entails the next\\nSome useful hints are: Based on the given input and output pairs, the general reason for non-entailment is that the hypothesis does not provide sufficient information to logically follow from the premise. The premises provide specific details or context that are not present in the hypothesis, or the hypothesis does not provide any new information or insights beyond what is already implied in the premise. Additionally, some of the premises provide subjective interpretations or assumptions, which do not necessarily imply factual or objective truth. Therefore, the output for all the given input pairs is expected to be non-entailment.\\nGiven input: <INPUT>\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "inp=\"<s>[INST] Determine whether one sentence entails the next\\nGiven input: Premise: Taylor assumes that Sophia believes that a man wearing a brown shirt and backpack is pushing a stroller while a small toddler is walking along aside him on a rain-covered street. Hypothesis: Taylor assumes that a man wearing a brown shirt and backpack is pushing a stroller while a small toddler is walking along aside him on a rain-covered street.\\n\n",
    "Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. [/INST]\"\n",
    "API_URL = \"https://api-inference.huggingface.co/models/google/flan-t5-xxl\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": inp,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'entailment'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
