{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_DATASETS_CACHE=\"/scratch/pt2295/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/pt2295/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"/scratch/pt2295/LlvmProject/automatic_prompt_engineer/data/bigbench-ii/epistemic_reasoning/task.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    \n",
    "    data_orig = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_orig['examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_orig['examples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "\n",
    "# Define the proportions for train, test, and validation sets\n",
    "total_samples = len(data)\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "validation_ratio = 0.15\n",
    "\n",
    "# Calculate the sizes of each set\n",
    "train_size = int(total_samples * train_ratio)\n",
    "test_size = int(total_samples * test_ratio)\n",
    "validation_size = int(total_samples * validation_ratio)\n",
    "\n",
    "# Split the data into sets\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:train_size + test_size]\n",
    "validation_data = data[train_size + test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Save train_data to train.json\n",
    "# with open('data/train_entail.json', 'w') as train_file:\n",
    "#     json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# # Save test_data to test.json\n",
    "# with open('data/test_entail.json', 'w') as test_file:\n",
    "#     json.dump(test_data, test_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/val_entail.json', 'w') as val_file:\n",
    "#     json.dump(validation_data, val_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read data from train.json\n",
    "with open('data/train_entail.json', 'r') as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "\n",
    "# Read data from test.json\n",
    "with open('data/test_entail.json', 'r') as test_file:\n",
    "    test_data = json.load(test_file)\n",
    "with open('data/val_entail.json', 'r') as val_file:\n",
    "    validation_data=json.load(val_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=train_data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Premise: Charles suspects that Taylor thinks that two guys on a couch, one is looking up the other is looking away with a cup in his hand. Hypothesis: Charles suspects that two guys on a couch, one is looking up the other is looking away with a cup in his hand.',\n",
       " 'target_scores': {'entailment': 0, 'non-entailment': 1}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description=data_orig['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type='entailment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix=data_orig['task_prefix']\n",
    "if task_type=='hyperbaton':\n",
    "    task_prefix=task_prefix.strip()\n",
    "    task_prefix+=\" Choose only from the following options: 'a' or 'b'.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(length,line,task_type='entailment'):\n",
    "    line=line[length:]\n",
    "    pattern = r'<Ans>(.*?)</Ans>|Ans:\\s*([\\w-]+)'\n",
    "    matches = re.findall(pattern, line)\n",
    "    if len(matches)==0:#right now if not match return\n",
    "        return -1\n",
    "    if task_type=='entailment':\n",
    "        if matches =='entailment' or matches=='Entailment' or \"non-entail\" not in matches:\n",
    "            return 1\n",
    "    elif task_type=='hyperbaton':\n",
    "        if matches[0][0]=='a' or matches[0][0]=='A':\n",
    "            return 1\n",
    "    return 0\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# custom_cache_dir = \"/scratch/pt2295/cache/\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=custom_cache_dir,use_fast=True)\n",
    "\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True,cache_dir=custom_cache_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def get_api(inp,is_summary=False):\n",
    "    def query(payload):\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        return response.json()\n",
    "    if is_summary==False:\n",
    "        output = query({\n",
    "            \"inputs\": inp,\n",
    "            \"parameters\":{\"max_new_tokens\":2048}\n",
    "        })\n",
    "    else:\n",
    "        output = query({\n",
    "            \"inputs\": inp,\n",
    "            \"parameters\":{\"max_new_tokens\":4096}\n",
    "        })\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_llm(user_prompt,use_api=False,is_summary=False):\n",
    "# user_prompt=\"Determine whether one sentence entails the next. Given input: Premise: William suspects that Isabella learns that two men are in blue life jackets and sitting on innertubes with 'Crusher' written on them in the middle of a body of water. Hypothesis: Isabella learns that two men are in blue life jackets and sitting on innertubes with 'Crusher' written on them in the middle of a body of water.Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'. Put your answer within tag <Ans> and </Ans>\"\n",
    "    sys_prompt=\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "    prompt=f\"<s>[INST] {user_prompt} [/INST]\"\n",
    "    \n",
    "    if use_api==False:\n",
    "   \n",
    "        model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        output = model.generate(**model_inputs)\n",
    "\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    else:\n",
    "\n",
    "        try:\n",
    "            return get_api(prompt,is_summary)[0]['generated_text']\n",
    "        except:\n",
    "            return \"\"\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "def run_inference_step1(prompt_t,use_api=False,task_type='entailment'):\n",
    "    wrong_ans_indices=[]\n",
    "    right_ans_indices=[]\n",
    "    positive_class=[]\n",
    "    negative_class=[]\n",
    "    not_got=[]\n",
    "    temp_prompt_t=prompt_t\n",
    "    for i in range(len(train_data)):\n",
    "        \n",
    "        prompt_t=temp_prompt_t.replace('<INPUT>',train_data[i]['input'])\n",
    "        op=get_answer_llm(prompt_t,use_api)\n",
    "        \n",
    "        if op=='':\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        output_str=op\n",
    "  \n",
    "        ans=extract_answer(len(prompt_t),output_str,task_type)\n",
    "        if ans==-1:\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        ground_truth=1\n",
    "        \n",
    "        if task_type=='entailment':\n",
    "            if train_data[i]['target_scores']['non-entailment']==1:\n",
    "                ground_truth=0\n",
    "        elif task_type=='hyperbaton':\n",
    "            if train_data[i]['target_scores']['b']==1:\n",
    "                ground_truth=0\n",
    "#         print(ans,ground_truth)\n",
    "#         print(train_data[i]['target_scores'])\n",
    "#         print(\"**********\")\n",
    "#         print(op)\n",
    "#         print('******')\n",
    "        if (ans!=ground_truth):\n",
    "            wrong_ans_indices.append(i)\n",
    "        else:\n",
    "            right_ans_indices.append(i)\n",
    "        if ground_truth==1:\n",
    "            positive_class.append(i)\n",
    "        else:\n",
    "            negative_class.append(i)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    return wrong_ans_indices,right_ans_indices,positive_class,negative_class,not_got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong_answers,not_got=run_inference_step1(prompt_t,True,task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer not only within tags, difficult to extract answer -> solved by improving prompt\n",
    "#70b is good but v slow\n",
    "#maybe use pipeline with datasets or use more gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hint(length,line):\n",
    "    line=line[length:]\n",
    "    pattern = r'<hint>(.*?)</hint>'\n",
    "    matches = re.findall(pattern, line)\n",
    "    if len(matches)==0:\n",
    "        return \"\"\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (wrong_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(not_got)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(wrong_answers,for_summary=True,k=20):\n",
    "    if for_summary==False:\n",
    "        k=max(k,math.ceil(0.4*len(wrong_answers)))\n",
    "    else:\n",
    "        k=max(k,math.ceil(0.4*len(wrong_answers)))\n",
    "        \n",
    "    print(\"k values is\", k)\n",
    "    \n",
    "    selected=random.sample(wrong_answers, k)\n",
    "    return selected\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hints_residual_step2(wrong_answers,right_answers,pos,neg,use_api=True):\n",
    "    hints_wrong_ans={}\n",
    "    l1=math.ceil(len(right_answers)*0.5)\n",
    "    l2=math.ceil(len(wrong_answers)*0.5)\n",
    "    print(\"positive and negative\",len(pos),len(neg))\n",
    "    selected_right_answers=random_sampling(right_answers,False,l1)#THIS STEP WAS NOT DONE IN PAPER BUT I DO BECAUSE HINTING TAKING TOO MUCH TIME\n",
    "    selected_wrong_answers=random_sampling(wrong_answers,False,l2)\n",
    "    selected_answers=selected_right_answers+selected_wrong_answers\n",
    "    \n",
    "    for idx in selected_answers:\n",
    "        if task_type=='entailment':\n",
    "            if train_data[idx]['target_scores']['entailment']==1:\n",
    "                ans='entailment'\n",
    "            else:\n",
    "                ans='non-entailment'\n",
    "        prompt_h='Given following task:'+task_description+'\\n'+'Given input: '+train_data[idx]['input']+'\\n'+'And its expected output: '+ans+'\\n'+'List the reason or hint why its with this expected output within tag <hint> and </hint>. The hint or explaination should be necassarily between the tags.'\n",
    "       \n",
    "        op=get_answer_llm(prompt_h,use_api)\n",
    "        \n",
    "\n",
    "        hint=extract_hint(len(prompt_h),op)\n",
    "        if idx%1000==0:\n",
    "            print(op,len(op))\n",
    "\n",
    "       \n",
    "        if hint!='':\n",
    "            hints_wrong_ans[idx]=hint\n",
    "        \n",
    "    return hints_wrong_ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hints_wrong=get_hints_residual_step2(wrong_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hints_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(hints_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarise_step4(hints_wrong,use_api=True):\n",
    "    selected=random_sampling(list(hints_wrong.keys()),True,len(hints_wrong)-1)#HERE I HAVED USED HINTS WRONG INSTEAD OF WRONG_ANS \n",
    "    prompt_s=\"This is a task to \"+task_description+\". We have some expected input and output pairs and have asked labeler to give reason or hint for each expected output. Given following data each contains input, output and reason for the expected output, summarize a general reason for all these cases:\"+'\\n'\n",
    "    temp_str=''\n",
    "    ctr=1\n",
    "    for idx in selected:\n",
    "        if len(temp_str)>=4095:\n",
    "            break\n",
    "        temp=''\n",
    "        if task_type=='entailment':\n",
    "            if train_data[idx]['target_scores']['entailment']==1:\n",
    "                ans='entailment'\n",
    "            else:\n",
    "                ans='non-entailment'\n",
    "\n",
    "        temp+='Given input: '+train_data[idx]['input']+'\\n'+'And its expected output: '+ans+'. And the reason for the expected output: '+hints_wrong[idx]+'\\n'\n",
    "        temp_str+=temp+'\\n'\n",
    "        \n",
    "    prompt_s+=temp_str+'\\n'+\"Summarised reason output:\"\n",
    "\n",
    "    l=len(prompt_s)\n",
    "    op=get_answer_llm(prompt_s,use_api,True)#[l+8+7:]#or extract after [/INSTR]\n",
    "   \n",
    "    op=op[len(prompt_s)+9+10:]\n",
    "    print(\"summarised op is\",op)\n",
    "    return op.lstrip()\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarised_prompt=get_summarise_step4(wrong_answers,hints_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_prompt=task_description+\"\\n\"+'Some useful hints are: '+summarised_prompt+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have to replicate all these steps for T iterations, just a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_iterations_T=10\n",
    "succesive_prompts=[]\n",
    "start=time.time()\n",
    "file_path = \"data/my_file_prapti_22032.txt\"\n",
    "prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "for t in range(num_iterations_T):\n",
    "    s1=time.time()\n",
    "    wrong_answers,right_answers,pos,neg,ng=run_inference_step1(prompt_t,task_type)\n",
    "    print(\"number of wrong_ans are\",len(wrong_answers),len(ng),len(right_answers))\n",
    "    print('inference finished in',time.time()-s1)\n",
    "    s2=time.time()\n",
    "    hints_wrong=get_hints_residual_step2(wrong_answers,right_answers,pos,neg)\n",
    "    print(\"num of hints\",len(hints_wrong))\n",
    "    print('hints finished in',time.time()-s2)\n",
    "    s3=time.time()\n",
    "    summarised_prompt=get_summarise_step4(hints_wrong)\n",
    "    print('summarisation finished in',time.time()-s3)\n",
    "    final_prompt=task_description+\"\\n\"+'Some useful hints are: '+summarised_prompt+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "    succesive_prompts.append(final_prompt)\n",
    "    prompt_t=final_prompt\n",
    "    print(prompt_t)\n",
    "    print(time.time()-start)\n",
    "    with open(file_path, \"a\") as file:\n",
    "    # Write the string to the file\n",
    "        file.write(prompt_t+\"\\n\"+\"Iteration\"+str(t)+\"\\n\")\n",
    "print(\"Time taken\",time.time()-start)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succesive_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_out(successive_prompts,use_api=True):\n",
    "    final_prompt_totest=successive_prompts[-1]\n",
    "    wrong_ans_indices=[]\n",
    "    not_got=[]\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        \n",
    "        prompt_t=final_prompt_totest.replace('<INPUT>',test_data[i]['input'])\n",
    "        op=get_answer_llm(prompt_t,use_api)\n",
    "        if op=='':\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        output_str=op\n",
    "\n",
    "        ans=extract_answer(len(prompt_t),output_str,task_type)\n",
    "        if ans==-1:\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        ground_truth=1\n",
    "      \n",
    "        if task_type=='entailment':\n",
    "            if test_data[i]['target_scores']['non-entailment']==1:\n",
    "                ground_truth=0\n",
    "        elif task_type=='hyperbaton':\n",
    "            if test_data[i]['target_scores']['b']==1:\n",
    "                ground_truth=0\n",
    "        if (ans!=ground_truth):\n",
    "            wrong_ans_indices.append(i)\n",
    "            \n",
    "    return wrong_ans_indices,not_got\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_ans_indice_test,not_got_test=test_out(successive_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wrong_ans_indice_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_got_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "140+59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
