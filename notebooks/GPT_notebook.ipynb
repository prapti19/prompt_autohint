{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_DATASETS_CACHE=\"/scratch/tg2520/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/tg2520/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = \"gpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "# file_path = \"/scratch/pt2295/LlvmProject/automatic_prompt_engineer/data/bigbench-ii/epistemic_reasoning/task.json\"\n",
    "file_path = \"/scratch/tg2520/my_env/LLVM/automatic_prompt_engineer/data/bigbench-ii/epistemic_reasoning/task.json\"\n",
    "with open(file_path, 'r') as json_file:\n",
    "    \n",
    "    data_orig = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_orig['examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_orig['examples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(data)\n",
    "\n",
    "# # Define the proportions for train, test, and validation sets\n",
    "# total_samples = len(data)\n",
    "# train_ratio = 0.7\n",
    "# test_ratio = 0.15\n",
    "# validation_ratio = 0.15\n",
    "\n",
    "# # Calculate the sizes of each set\n",
    "# train_size = int(total_samples * train_ratio)\n",
    "# test_size = int(total_samples * test_ratio)\n",
    "# validation_size = int(total_samples * validation_ratio)\n",
    "\n",
    "# # Split the data into sets\n",
    "# train_data = data[:train_size]\n",
    "# test_data = data[train_size:train_size + test_size]\n",
    "# validation_data = data[train_size + test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read data from train.json\n",
    "with open('train_entail.json', 'r') as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "\n",
    "# Read data from test.json\n",
    "with open('test_entail.json', 'r') as test_file:\n",
    "    test_data = json.load(test_file)\n",
    "with open('val_entail.json', 'r') as val_file:\n",
    "    validation_data=json.load(val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 1400\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data size: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Premise: Charles suspects that Taylor thinks that two guys on a couch, one is looking up the other is looking away with a cup in his hand. Hypothesis: Charles suspects that two guys on a couch, one is looking up the other is looking away with a cup in his hand.',\n",
       " 'target_scores': {'entailment': 0, 'non-entailment': 1}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "pos_train = [i for i in range(len(train_data)) if train_data[i]['target_scores']['entailment'] == 1]\n",
    "neg_train = [i for i in range(len(train_data)) if train_data[i]['target_scores']['non-entailment'] == 1]\n",
    "\n",
    "print(len(pos_train))\n",
    "print(len(neg_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determine whether one sentence entails the next\n"
     ]
    }
   ],
   "source": [
    "task_description=data_orig['description']\n",
    "print(task_description)\n",
    "\n",
    "task_type='entailment'\n",
    "\n",
    "task_prefix=data_orig['task_prefix']\n",
    "if task_type=='hyperbaton':\n",
    "    task_prefix=task_prefix.strip()\n",
    "    task_prefix+=\" Choose only from the following options: 'a' or 'b'.\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GPT API CALL\n",
    "\n",
    "# GPT-4\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = \"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\")\n",
    "#gpt-3.5-turbo\n",
    "#gpt-4-0613\n",
    "def get_openai_api(inp, temp=0,topP=1):\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-0613\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": inp}\n",
    "        ],\n",
    "        temperature = temp,\n",
    "        top_p = topP\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_llm(user_prompt,use_api=False,temp=0,topP=1):\n",
    "# user_prompt=\"Determine whether one sentence entails the next. Given input: Premise: William suspects that Isabella learns that two men are in blue life jackets and sitting on innertubes with 'Crusher' written on them in the middle of a body of water. Hypothesis: Isabella learns that two men are in blue life jackets and sitting on innertubes with 'Crusher' written on them in the middle of a body of water.Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'. Put your answer within tag <Ans> and </Ans>\"\n",
    "    sys_prompt=\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "    prompt = \"\"\n",
    "    if LLM == 'llama':\n",
    "        prompt=f\"<s>[INST] {user_prompt} [/INST]\"\n",
    "        \n",
    "        if use_api==False:\n",
    "            model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "            output = model.generate(**model_inputs)\n",
    "            return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        else:\n",
    "\n",
    "            try:\n",
    "                return get_api(prompt,is_summary)[0]['generated_text']\n",
    "            except:\n",
    "                return \"\"\n",
    "    elif LLM == 'gpt':\n",
    "        prompt = user_prompt\n",
    "        try:\n",
    "            return get_openai_api(prompt)\n",
    "        except:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(length,line,task_type='entailment'):\n",
    "    line=line[length:]\n",
    "    pattern = r'<Ans>(.*?)</Ans>|Ans:\\s*([\\w-]+)'\n",
    "    matches = re.findall(pattern, line)\n",
    "    if len(matches)==0:#right now if not match return\n",
    "        return -1\n",
    "    if task_type=='entailment':\n",
    "        if matches =='entailment' or matches=='Entailment' or \"non-entail\" not in matches:\n",
    "            return 1\n",
    "    elif task_type=='hyperbaton':\n",
    "        if matches[0][0]=='a' or matches[0][0]=='A':\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hint(length,line):\n",
    "    line=line[length:]\n",
    "    pattern = r'<hint>(.*?)</hint>'\n",
    "    matches = re.findall(pattern, line)\n",
    "    if len(matches)==0:\n",
    "        return \"\"\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial prompt\n",
    "prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Get inferences for the entire dataset (y_hat <- {xi,yi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_step1(prompt_t,use_api=True,task_type='entailment'):\n",
    "    wrong_ans_indices=[] #wrong samples\n",
    "    correct_ans_indices=[] #correct samples\n",
    "    positive_class=[] #entailment class\n",
    "    negative_class=[] #non-entailment class\n",
    "    not_got=[]\n",
    "    temp_prompt_t=prompt_t\n",
    "    for i in range(len(train_data)):        \n",
    "        prompt_t=temp_prompt_t.replace('<INPUT>',train_data[i]['input'])\n",
    "        op=get_answer_llm(prompt_t,use_api)\n",
    "        if op=='':\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        output_str=op\n",
    "        \n",
    "        ans=extract_answer(len(prompt_t) if LLM==\"llama\" else 0,output_str,task_type)\n",
    "        if ans==-1:\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        ground_truth=1\n",
    "      \n",
    "        if task_type=='entailment':\n",
    "            if train_data[i]['target_scores']['non-entailment']==1:\n",
    "                ground_truth=0\n",
    "        elif task_type=='hyperbaton':\n",
    "            if train_data[i]['target_scores']['b']==1:\n",
    "                ground_truth=0\n",
    "                \n",
    "        # Sampling        \n",
    "        if (ans!=ground_truth):\n",
    "            wrong_ans_indices.append(i)\n",
    "        else:\n",
    "            correct_ans_indices.append(i)\n",
    "        \n",
    "        if ground_truth==1:\n",
    "            positive_class.append(i)\n",
    "        else:\n",
    "            negative_class.append(i)\n",
    "    \n",
    "    return wrong_ans_indices,correct_ans_indices, positive_class,negative_class,not_got\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Sampling (Done before summarization in the original paper, but we are doing this before generating hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(wrong_answers,k=3):\n",
    "    selected=random.sample(wrong_answers, k)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_balanced_sampling(wrong_answers,right_answers,pos,neg,k=3):\n",
    "    #sample from wrong ans\n",
    "    wrong_pos_indices = [i for i in wrong_answers if i in pos]\n",
    "    wrong_neg_indices = [i for i in wrong_answers if i in neg]\n",
    "\n",
    "    random_sample_pos = random.sample(wrong_pos_indices, k if len(wrong_pos_indices)>=k else len(wrong_pos_indices))\n",
    "    random_sample_neg = random.sample(wrong_neg_indices, k if len(wrong_neg_indices)>=k else len(wrong_neg_indices))\n",
    "    \n",
    "    if len(random_sample_pos) < k:\n",
    "        random_sample_pos += random.sample(pos_train, k - len(random_sample_pos))\n",
    "    \n",
    "    if len(random_sample_neg) < k:\n",
    "        random_sample_neg.append += random.sample(neg_train, k - len(random_sample_neg))\n",
    "        \n",
    "    print(f\"random_sample_pos(Wrong ans) : {len(random_sample_pos)}\")\n",
    "    print(f\"random_sample_neg(Wrong ans) : {len(random_sample_neg)}\")\n",
    "    return random_sample_pos + random_sample_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Extract hints from the samples <br> temperature as 0.1 and topP as 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hints_residual_step2(answers,use_api=True):\n",
    "    hints={}\n",
    "\n",
    "    for idx in answers:\n",
    "        if task_type=='entailment':\n",
    "            if train_data[idx]['target_scores']['entailment']==1:\n",
    "                ans='entailment'\n",
    "            else:\n",
    "                ans='non-entailment'\n",
    "        prompt_h='Given following task:'+task_description+'\\n'+'Given input: '+train_data[idx]['input']+'\\n'+'And its expected output: '+ans+'\\n'+'List the reason or hint why its with this expected output within tag <hint> and </hint>. The hint or explaination should be necassarily between the tags.'\n",
    "       \n",
    "        op=get_answer_llm(prompt_h,use_api, temp=0.1,topP=0.95)\n",
    "        hint=extract_hint(len(prompt_h) if LLM==\"llama\" else 0,op)\n",
    "        \n",
    "        if idx%1000==0:\n",
    "            print(op,len(op))\n",
    "\n",
    "       \n",
    "        if hint!='':\n",
    "            hints[idx]=hint\n",
    "        \n",
    "    return hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Summarize the hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarise_step4(hints,use_api=True,):\n",
    "    prompt_s=\"This is a task to \"+task_description+\". We have some expected input and output pairs and have asked labeler to give reason or hint for each expected output. Given following data each contains input, output and reason for the expected output, summarize a general reason for all these cases:\"+'\\n'\n",
    "    temp_str=''\n",
    "    ctr=1\n",
    "    for idx in hints:\n",
    "        temp=''\n",
    "        if task_type=='entailment':\n",
    "            if train_data[idx]['target_scores']['entailment']==1:\n",
    "                ans='entailment'\n",
    "            else:\n",
    "                ans='non-entailment'\n",
    "\n",
    "        temp+='Given input: '+train_data[idx]['input']+'\\n'+'And its expected output: '+ans+'. And the reason for the expected output: '+hints[idx]+'\\n'\n",
    "        temp_str+=temp+'\\n'\n",
    "        \n",
    "#     prompt_s+=temp_str+'\\n'+\"Summarised reason output:\"\n",
    "    \n",
    "    prompt_s+=temp_str+'\\n'+\"Give a summary of the reasons for the example output, and do not give a reason particular to the respective example. Also do not mention the number of examples nor give any reference to entities in examples in the summary directly or indirectly. Be as general as possible. The summarised reasons are:\"\n",
    "\n",
    "    op=get_answer_llm(prompt_s,use_api)#[l+8+7:]#or extract after [/INSTR]\n",
    "    return op.lstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run for 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Ans: 72\n",
      "Correct Ans: 28\n",
      "inference finished in 160.52822852134705\n",
      "random_sample_pos(Wrong ans) : 3\n",
      "random_sample_neg(Wrong ans) : 3\n",
      "[80, 76, 26, 2, 78, 22]\n",
      "Hints finished in 58.218313694000244\n",
      "summarisation finished in 14.4967041015625\n",
      "233.24367213249207\n",
      "Wrong Ans: 72\n",
      "Correct Ans: 28\n",
      "inference finished in 168.54954266548157\n",
      "random_sample_pos(Wrong ans) : 3\n",
      "random_sample_neg(Wrong ans) : 3\n",
      "[75, 27, 12, 15, 87, 82]\n",
      "Hints finished in 52.44496130943298\n",
      "summarisation finished in 12.925204277038574\n",
      "467.1639142036438\n",
      "Wrong Ans: 72\n",
      "Correct Ans: 28\n",
      "inference finished in 173.14038920402527\n",
      "random_sample_pos(Wrong ans) : 3\n",
      "random_sample_neg(Wrong ans) : 3\n",
      "[14, 92, 35, 71, 60, 43]\n",
      "Hints finished in 55.61220598220825\n",
      "summarisation finished in 13.709689378738403\n",
      "709.6266725063324\n",
      "Wrong Ans: 72\n",
      "Correct Ans: 28\n",
      "inference finished in 166.24409866333008\n",
      "random_sample_pos(Wrong ans) : 3\n",
      "random_sample_neg(Wrong ans) : 3\n",
      "[95, 50, 75, 36, 71, 13]\n",
      "Hints finished in 46.22351098060608\n",
      "summarisation finished in 15.061739683151245\n",
      "937.1564767360687\n",
      "Wrong Ans: 72\n",
      "Correct Ans: 28\n",
      "inference finished in 174.52099299430847\n",
      "random_sample_pos(Wrong ans) : 3\n",
      "random_sample_neg(Wrong ans) : 3\n",
      "[56, 21, 77, 58, 41, 78]\n",
      "Hints finished in 52.637295722961426\n",
      "summarisation finished in 18.359936237335205\n",
      "1182.679365158081\n",
      "Wrong Ans: 72\n",
      "Correct Ans: 28\n",
      "inference finished in 177.2643358707428\n",
      "random_sample_pos(Wrong ans) : 3\n",
      "random_sample_neg(Wrong ans) : 3\n",
      "[50, 52, 64, 4, 69, 25]\n",
      "Hints finished in 48.16255569458008\n",
      "summarisation finished in 16.889398097991943\n",
      "1424.9961152076721\n",
      "Wrong Ans: 72\n",
      "Correct Ans: 28\n",
      "inference finished in 184.21260952949524\n",
      "random_sample_pos(Wrong ans) : 3\n",
      "random_sample_neg(Wrong ans) : 3\n",
      "[27, 96, 64, 81, 65, 88]\n",
      "Hints finished in 57.30666184425354\n",
      "summarisation finished in 10.801628828048706\n",
      "1677.3187754154205\n",
      "Wrong Ans: 72\n",
      "Correct Ans: 28\n",
      "inference finished in 160.88391709327698\n",
      "random_sample_pos(Wrong ans) : 3\n",
      "random_sample_neg(Wrong ans) : 3\n",
      "[50, 68, 64, 18, 39, 47]\n",
      "Hints finished in 52.177571058273315\n",
      "summarisation finished in 11.570127964019775\n",
      "1901.9509773254395\n",
      "Wrong Ans: 72\n",
      "Correct Ans: 28\n",
      "inference finished in 175.65009927749634\n",
      "random_sample_pos(Wrong ans) : 3\n",
      "random_sample_neg(Wrong ans) : 3\n",
      "[77, 34, 52, 36, 25, 74]\n",
      "Hints finished in 53.327361822128296\n",
      "summarisation finished in 15.43645167350769\n",
      "2146.3653326034546\n",
      "Wrong Ans: 72\n",
      "Correct Ans: 28\n",
      "inference finished in 185.1180546283722\n",
      "random_sample_pos(Wrong ans) : 3\n",
      "random_sample_neg(Wrong ans) : 3\n",
      "[26, 34, 50, 25, 15, 90]\n",
      "Hints finished in 50.464492321014404\n",
      "summarisation finished in 14.249163150787354\n",
      "2396.1974596977234\n",
      "Time taken 2396.1979019641876\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_iterations_T=10\n",
    "succesive_prompts=[]\n",
    "start=time.time()\n",
    "file_path = \"results_0911.txt\"\n",
    "prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "for t in range(num_iterations_T):\n",
    "    # Step 1: Inference\n",
    "    s1=time.time()\n",
    "    wrong_answers,right_answers,pos,neg,ng=run_inference_step1(prompt_t,task_type)\n",
    "    print(f\"Wrong Ans: {len(wrong_answers)}\")\n",
    "    print(f\"Correct Ans: {len(right_answers)}\")\n",
    "    print('inference finished in',time.time()-s1)\n",
    "    \n",
    "    # Step 2: Sampling\n",
    "    samples = random_balanced_sampling(wrong_answers,right_answers,pos,neg)\n",
    "    print(samples)\n",
    "    \n",
    "    # Step 3: Generate Hints\n",
    "    s2=time.time()\n",
    "    hints=get_hints_residual_step2(samples)\n",
    "    print('Hints finished in',time.time()-s2)\n",
    "    \n",
    "    # Step 4: Summarize Hints\n",
    "    s3=time.time()\n",
    "    summarised_prompt=get_summarise_step4(hints)\n",
    "    print('summarisation finished in',time.time()-s3)\n",
    "    \n",
    "    \n",
    "    final_prompt=task_description+\"\\n\"+'Some useful hints are: '+summarised_prompt+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "    succesive_prompts.append(final_prompt)\n",
    "    prompt_t=final_prompt\n",
    "    \n",
    "    print(time.time()-start)\n",
    "    with open(file_path, \"a\") as file:\n",
    "    # Write the string to the file\n",
    "        file.write(prompt_t+\"\\n\"+\"Iteration\"+str(t)+\"\\n\")\n",
    "print(\"Time taken\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_out(final_prompt_totest,use_api=True):\n",
    "    wrong_ans_indices=[]\n",
    "    not_got=[]\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        \n",
    "        prompt_t=final_prompt_totest.replace('<INPUT>',test_data[i]['input'])\n",
    "        op=get_answer_llm(prompt_t,use_api)\n",
    "        if op=='':\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        output_str=op\n",
    "\n",
    "        ans=extract_answer(len(prompt_t) if LLM==\"llama\" else 0,output_str,task_type)\n",
    "        if ans==-1:\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        ground_truth=1\n",
    "      \n",
    "        if task_type=='entailment':\n",
    "            if test_data[i]['target_scores']['non-entailment']==1:\n",
    "                ground_truth=0\n",
    "        elif task_type=='hyperbaton':\n",
    "            if test_data[i]['target_scores']['b']==1:\n",
    "                ground_truth=0\n",
    "        if (ans!=ground_truth):\n",
    "            wrong_ans_indices.append(i)\n",
    "            \n",
    "    return wrong_ans_indices,not_got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Determine whether one sentence entails the next\\nSome useful hints are: The entailment or non-entailment between a premise and a hypothesis depends on the information shared between them. If the hypothesis is a simplified version of the premise, containing all the information from the premise but without additional details, then the premise entails the hypothesis. However, if the premise only suggests or suspects something about another person's knowledge or belief, it does not confirm that the person actually holds that knowledge or belief. Therefore, the premise does not necessarily entail the hypothesis in such cases.\\nGiven input: <INPUT>\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "succesive_prompts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Determine whether one sentence entails the next\\nGiven input: <INPUT>\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "prompt_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_ans_indice_test,not_got_test=test_out(prompt_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_ans_indice_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_got_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
