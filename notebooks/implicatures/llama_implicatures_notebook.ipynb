{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_DATASETS_CACHE=\"/scratch/tg2520/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/tg2520/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM = \"llama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "\n",
    "file_path = \"/scratch/tg2520/my_env/LLVM/automatic_prompt_engineer/data/bigbench-ii/implicatures/task.json\"\n",
    "# file_path = \"/scratch/tg2520/my_env/LLVM/automatic_prompt_engineer/data/bigbench-ii/epistemic_reasoning/task.json\"\n",
    "with open(file_path, 'r') as json_file:\n",
    "    \n",
    "    data_orig = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_orig['examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_orig['examples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n",
      "246\n"
     ]
    }
   ],
   "source": [
    "pos_datapoints = [i for i in range(len(data)) if data[i]['target_scores']['yes'] == 1.0]\n",
    "neg_datapoints = [i for i in range(len(data)) if data[i]['target_scores']['no'] == 1.0]\n",
    "\n",
    "print(len(pos_datapoints))\n",
    "print(len(neg_datapoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.shuffle(data)\n",
    "\n",
    "# # Define the proportions for train, test, and validation sets\n",
    "# total_samples = len(data)\n",
    "# train_ratio = 0.8\n",
    "# test_ratio = 0.1\n",
    "# validation_ratio = 0.1\n",
    "\n",
    "# # Calculate the sizes of each set\n",
    "# train_size = int(total_samples * train_ratio)\n",
    "# test_size = int(total_samples * test_ratio)\n",
    "# validation_size = int(total_samples * validation_ratio)\n",
    "\n",
    "# # Split the data into sets\n",
    "# train_data = data[:train_size]\n",
    "# test_data = data[train_size:train_size + test_size]\n",
    "# validation_data = data[train_size + test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('train_implicature.json', 'w') as json_file:\n",
    "#     json.dump(train_data, json_file)\n",
    "    \n",
    "# with open('test_implicature.json', 'w') as json_file:\n",
    "#     json.dump(test_data, json_file)\n",
    "\n",
    "# with open('val_implicature.json', 'w') as json_file:\n",
    "#     json.dump(validation_data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read data from train.json\n",
    "with open('train_implicature.json', 'r') as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "\n",
    "# Read data from test.json\n",
    "with open('test_implicature.json', 'r') as test_file:\n",
    "    test_data = json.load(test_file)\n",
    "with open('val_implicature.json', 'r') as val_file:\n",
    "    validation_data=json.load(val_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 393\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data size: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=train_data[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"Speaker 1: 'Do you think you can give me a ride home?' Speaker 2: 'A friend in need is a friend indeed.'\",\n",
       " 'target_scores': {'yes': 1.0, 'no': 0.0}}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "197\n"
     ]
    }
   ],
   "source": [
    "pos_train = [i for i in range(len(train_data)) if train_data[i]['target_scores']['yes'] == 1.0]\n",
    "neg_train = [i for i in range(len(train_data)) if train_data[i]['target_scores']['no'] == 1.0]\n",
    "\n",
    "print(len(pos_train))\n",
    "print(len(neg_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict whether Speaker 2's answer to Speaker 1 counts as a yes or as a no\n",
      "Does Speaker 2's answer mean yes or no?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task_description=data_orig['description']\n",
    "print(task_description)\n",
    "\n",
    "task_type='implicatures'\n",
    "\n",
    "task_prefix=data_orig['task_prefix']\n",
    "if task_type=='hyperbaton':\n",
    "    task_prefix=task_prefix.strip()\n",
    "    task_prefix+=\" Choose only from the following options: 'a' or 'b'.\\n\"\n",
    "    \n",
    "print(task_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path2 = \"results_implicature_llama_2311.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GPT API CALL\n",
    "\n",
    "# GPT-4\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = \"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\")\n",
    "#gpt-3.5-turbo\n",
    "#gpt-4-0613\n",
    "def get_openai_api(inp, temp=0,topP=1):\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": inp}\n",
    "        ],\n",
    "        temperature = temp,\n",
    "        top_p = topP\n",
    "    )\n",
    "    \n",
    "    with open(file_path2, \"a\") as file2:\n",
    "    # Write the string to the file\n",
    "        file2.write(completion.choices[0].message.content+\"\\n\")\n",
    "    return completion.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama API call\n",
    "\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "# 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\n",
    "def get_llama_api(inp,is_summary=False):\n",
    "    def query(payload):\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        return response.json()\n",
    "    output = None\n",
    "    if is_summary==False:\n",
    "        output = query({\n",
    "            \"inputs\": inp,\n",
    "            \"parameters\":{\"max_new_tokens\":2048}\n",
    "        })\n",
    "    else:\n",
    "        output = query({\n",
    "            \"inputs\": inp,\n",
    "            \"parameters\":{\"max_new_tokens\":4096}\n",
    "        })\n",
    "#     print(output)\n",
    "#     print(output[0]['generated_text'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_llm(user_prompt,use_api=True,is_summary=False,temp=0,topP=1):\n",
    "# user_prompt=\"Determine whether one sentence entails the next. Given input: Premise: William suspects that Isabella learns that two men are in blue life jackets and sitting on innertubes with 'Crusher' written on them in the middle of a body of water. Hypothesis: Isabella learns that two men are in blue life jackets and sitting on innertubes with 'Crusher' written on them in the middle of a body of water.Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'. Put your answer within tag <Ans> and </Ans>\"\n",
    "    sys_prompt=\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "    prompt = \"\"\n",
    "    if LLM == 'llama':\n",
    "        prompt=f\"<s>[INST] {user_prompt} [/INST]\"\n",
    "        return get_llama_api(prompt,is_summary)[0]['generated_text']\n",
    "  \n",
    "    elif LLM == 'gpt':\n",
    "        prompt = user_prompt\n",
    "        return get_openai_api(prompt)\n",
    "#         try:\n",
    "#             return get_openai_api(prompt)\n",
    "#         except:\n",
    "#             return \"Error!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_prompt = \"Predict whether Speaker 2's answer to Speaker 1 counts as a yes or as a no. Speaker 1: 'Should I convince these clients?' Speaker 2: 'These are really important clients with deep pockets.' Does Speaker 2's answer mean yes or no? Choose only from the following options: 'yes' or 'no'.\\n Put your answer within tag <Ans> and </Ans>\"\n",
    "# print(user_prompt)\n",
    "ans = get_answer_llm(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s>[INST] Predict whether Speaker 2's answer to Speaker 1 counts as a yes or as a no. Speaker 1: 'But aren't you afraid?' Speaker 2: 'Ma'am, sharks never attack anybody.' Does Speaker 2's answer mean yes or no? Choose only from the following options: 'yes' or 'no'.\\n Put your answer within tag <Ans> and </Ans> [/INST]  <Ans>No</Ans>\\n\\nSpeaker 2's answer does not directly address Speaker 1's question about fear. Instead, Speaker 2 makes a statement that sharks never attack anybody. This can be interpreted as a way of downplaying the risk of shark attacks, but it does not explicitly answer the question about fear. Therefore, Speaker 2's answer can be considered a no.\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(length,line):\n",
    "    line=line[length:]\n",
    "    pattern = r'<Ans>(.*?)</Ans>|Ans:\\s*([\\w-]+)'\n",
    "    matches = re.findall(pattern, line)\n",
    "    if len(matches)==0:#right now if not match return\n",
    "        return -1\n",
    "    \n",
    "    matches=matches[0][0]\n",
    "    if task_type=='entailment':\n",
    "        if matches =='entailment' or matches=='Entailment':\n",
    "            return 1\n",
    "    elif task_type=='hyperbaton':\n",
    "        if matches[0][0]=='a' or matches[0][0]=='A':\n",
    "            return 1\n",
    "    elif task_type == 'implicatures':\n",
    "         if matches[0][0]=='yes' or matches[0][0]=='Yes':\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hint(length,line):\n",
    "    line=line[length:]\n",
    "    pattern = r'<hint>(.*?)</hint>'\n",
    "    matches = re.findall(pattern, line)\n",
    "    if len(matches)==0:\n",
    "        return \"\"\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_confidence(length,line):\n",
    "    line=line[length:]\n",
    "    pattern = r'<conf>(.*?)</conf>'\n",
    "    matches = re.findall(pattern, line)\n",
    "    if len(matches)==0:\n",
    "        return \"\"\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial prompt\n",
    "prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Get inferences for the entire dataset (y_hat <- {xi,yi})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_step1(prompt_t,use_api=True):\n",
    "    wrong_ans_indices=[] #wrong samples\n",
    "    correct_ans_indices=[] #correct samples\n",
    "    positive_class=[] #entailment class\n",
    "    negative_class=[] #non-entailment class\n",
    "    not_got=[]\n",
    "    conf = []\n",
    "    temp_prompt_t=prompt_t\n",
    "    for i in range(len(train_data)):        \n",
    "        prompt_t=temp_prompt_t.replace('<INPUT>',train_data[i]['input'])\n",
    "        op=get_answer_llm(prompt_t,use_api)\n",
    "        if op=='':\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        output_str=op\n",
    "        \n",
    "        ans=extract_answer(len(prompt_t) if LLM==\"llama\" else 0,output_str)\n",
    "        conf.append(extract_confidence(len(prompt_t) if LLM==\"llama\" else 0,output_str))\n",
    "        if ans==-1:\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        ground_truth=1\n",
    "      \n",
    "        if task_type=='entailment':\n",
    "            if train_data[i]['target_scores']['non-entailment']==1:\n",
    "                ground_truth=0\n",
    "        elif task_type=='hyperbaton':\n",
    "            if train_data[i]['target_scores']['b']==1:\n",
    "                ground_truth=0\n",
    "        elif task_type=='implicatures':\n",
    "            if train_data[i]['target_scores']['no']==1.0:\n",
    "                ground_truth=0\n",
    "                \n",
    "        # Sampling        \n",
    "        if (ans!=ground_truth):\n",
    "            wrong_ans_indices.append(i)\n",
    "        else:\n",
    "            correct_ans_indices.append(i)\n",
    "        \n",
    "        if ground_truth==1:\n",
    "            positive_class.append(i)\n",
    "        else:\n",
    "            negative_class.append(i)\n",
    "    \n",
    "    return wrong_ans_indices,correct_ans_indices, positive_class,negative_class,not_got\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Sampling (Done before summarization in the original paper, but we are doing this before generating hints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(wrong_answers,k=3):\n",
    "    print(len(wrong_answers))\n",
    "    selected=random.sample(wrong_answers, k)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_balanced_sampling(wrong_answers,right_answers,pos,neg,k=3):\n",
    "    #sample from wrong ans\n",
    "    wrong_pos_indices = [i for i in wrong_answers if i in pos]\n",
    "    wrong_neg_indices = [i for i in wrong_answers if i in neg]\n",
    "\n",
    "    random_sample_pos = random.sample(wrong_pos_indices, k if len(wrong_pos_indices)>=k else len(wrong_pos_indices))\n",
    "    random_sample_neg = random.sample(wrong_neg_indices, k if len(wrong_neg_indices)>=k else len(wrong_neg_indices))\n",
    "    \n",
    "    if len(random_sample_pos) < k:\n",
    "        random_sample_pos += random.sample(pos_train, k - len(random_sample_pos))\n",
    "    \n",
    "    if len(random_sample_neg) < k:\n",
    "        random_sample_neg.append += random.sample(neg_train, k - len(random_sample_neg))\n",
    "        \n",
    "    print(f\"random_sample_pos(Wrong ans) : {len(random_sample_pos)}\")\n",
    "    print(f\"random_sample_neg(Wrong ans) : {len(random_sample_neg)}\")\n",
    "    return random_sample_pos + random_sample_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Extract hints from the samples <br> temperature as 0.1 and topP as 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hints_residual_step2(answers,use_api=True):\n",
    "    hints={}\n",
    "\n",
    "    for idx in answers:\n",
    "        if task_type=='entailment':\n",
    "            if train_data[idx]['target_scores']['entailment']==1:\n",
    "                ans='entailment'\n",
    "            else:\n",
    "                ans='non-entailment'\n",
    "        if task_type=='implicatures':\n",
    "            if train_data[idx]['target_scores']['yes']==1.0:\n",
    "                ans='yes'\n",
    "            else:\n",
    "                ans='no'\n",
    "        prompt_h='Given following task:'+task_description+'\\n'+'Given input: '+train_data[idx]['input']+'\\n'+'And its expected output: '+ans+'\\n'+'List the reason or hint why its with this expected output within tag <hint> and </hint>. The hint or explaination should be necassarily between the tags.'\n",
    "       \n",
    "        op=get_answer_llm(prompt_h,use_api, temp=0.1,topP=0.95)\n",
    "        hint=extract_hint(len(prompt_h) if LLM==\"llama\" else 0,op)\n",
    "        \n",
    "        if idx%1000==0:\n",
    "            print(op,len(op))\n",
    "\n",
    "       \n",
    "        if hint!='':\n",
    "            hints[idx]=hint\n",
    "        \n",
    "    return hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Summarize the hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarise_step4(hints,use_api=True,):\n",
    "    prompt_s=\"This is a task to \"+task_description+\". We have some expected input and output pairs and have asked labeler to give reason or hint for each expected output. Given following data each contains input, output and reason for the expected output, summarize a general reason for all these cases:\"+'\\n'\n",
    "    temp_str=''\n",
    "    ctr=1\n",
    "    for idx in hints:\n",
    "        temp=''\n",
    "        if task_type=='entailment':\n",
    "            if train_data[idx]['target_scores']['entailment']==1:\n",
    "                ans='entailment'\n",
    "            else:\n",
    "                ans='non-entailment'\n",
    "        if task_type=='implicatures':\n",
    "            if train_data[idx]['target_scores']['yes']==1.0:\n",
    "                ans='yes'\n",
    "            else:\n",
    "                ans='no'\n",
    "\n",
    "        temp+='Given input: '+train_data[idx]['input']+'\\n'+'And its expected output: '+ans+'. And the reason for the expected output: '+hints[idx]+'\\n'\n",
    "        temp_str+=temp+'\\n'\n",
    "        \n",
    "#     prompt_s+=temp_str+'\\n'+\"Summarised reason output:\"\n",
    "    \n",
    "    prompt_s+=temp_str+'\\n'+\"Give a summary of the reasons for the example output, and do not give a reason particular to the respective example. Also do not mention the number of examples nor give any reference to entities in examples in the summary directly or indirectly. Be as general as possible. The summarised reasons are:\"\n",
    "\n",
    "    op=get_answer_llm(prompt_s,use_api)#[l+8+7:]#or extract after [/INSTR]\n",
    "    return op.lstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run for 10 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. Also provide the confidence score for the answer within tag <conf> and </conf>'\n",
    "prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_t= \"Determine whether one sentence entails the next\\nSome useful hints are: The entailment or non-entailment between a premise and a hypothesis depends on the information shared between them. If the hypothesis is a simplified version of the premise, containing all the information from the premise but without additional details, then the premise entails the hypothesis. However, if the premise only suggests or suspects something about another person's knowledge or belief, it does not confirm that the person actually holds that knowledge or belief. Therefore, the premise does not necessarily entail the hypothesis in such cases.\\nGiven input: <INPUT>\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags. Also give a confidence score between 0 and 1 representing the certainity of your answer and put your confidence score between <conf> and </conf> tags.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Predict whether Speaker 2's answer to Speaker 1 counts as a yes or as a no\\nGiven input: <INPUT>\\nDoes Speaker 2's answer mean yes or no?\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Ans: 196\n",
      "Correct Ans: 197\n",
      "Not got: 0\n",
      "inference finished in 1446.777615070343\n",
      "196\n",
      "[254, 171, 244]\n",
      "Hints finished in 24.979721546173096\n",
      "summarisation finished in 15.64814829826355\n",
      "1487.4061181545258\n",
      "Wrong Ans: 196\n",
      "Correct Ans: 197\n",
      "Not got: 0\n",
      "inference finished in 2094.3981840610504\n",
      "196\n",
      "[94, 23, 226]\n",
      "Hints finished in 34.25879406929016\n",
      "summarisation finished in 8.72301459312439\n",
      "3624.787853240967\n",
      "Wrong Ans: 196\n",
      "Correct Ans: 197\n",
      "Not got: 0\n",
      "inference finished in 1982.5988738536835\n",
      "196\n",
      "[350, 221, 130]\n",
      "Hints finished in 18.609025716781616\n",
      "summarisation finished in 13.578725814819336\n",
      "5639.579477310181\n",
      "Wrong Ans: 196\n",
      "Correct Ans: 197\n",
      "Not got: 0\n",
      "inference finished in 2204.4501481056213\n",
      "196\n",
      "[358, 337, 51]\n",
      "Hints finished in 23.862639665603638\n",
      "summarisation finished in 11.632674217224121\n",
      "7879.52565574646\n",
      "Wrong Ans: 196\n",
      "Correct Ans: 197\n",
      "Not got: 0\n",
      "inference finished in 2090.5812044143677\n",
      "196\n",
      "[246, 217, 67]\n",
      "Hints finished in 21.299473524093628\n",
      "summarisation finished in 7.638278961181641\n",
      "9999.045217037201\n",
      "Wrong Ans: 196\n",
      "Correct Ans: 197\n",
      "Not got: 0\n",
      "inference finished in 2041.8986666202545\n",
      "196\n",
      "[30, 281, 103]\n",
      "Hints finished in 22.01137065887451\n",
      "summarisation finished in 8.250421285629272\n",
      "12071.20638537407\n",
      "Wrong Ans: 196\n",
      "Correct Ans: 197\n",
      "Not got: 0\n",
      "inference finished in 2201.874473810196\n",
      "196\n",
      "[339, 42, 138]\n",
      "Hints finished in 25.821351528167725\n",
      "summarisation finished in 9.557292461395264\n",
      "14308.459976911545\n",
      "Wrong Ans: 196\n",
      "Correct Ans: 197\n",
      "Not got: 0\n",
      "inference finished in 2426.085644721985\n",
      "196\n",
      "[17, 276, 192]\n",
      "Hints finished in 23.74556589126587\n",
      "summarisation finished in 10.369094848632812\n",
      "16768.660972356796\n",
      "Wrong Ans: 196\n",
      "Correct Ans: 197\n",
      "Not got: 0\n",
      "inference finished in 2480.0174515247345\n",
      "196\n",
      "[276, 172, 388]\n",
      "Hints finished in 14.91085433959961\n",
      "summarisation finished in 8.617149591445923\n",
      "19272.207132816315\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations_T):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Step 1: Inference\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     s1\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m     wrong_answers,right_answers,pos,neg,ng\u001b[38;5;241m=\u001b[39mrun_inference_step1(prompt_t)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong Ans: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(wrong_answers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrect Ans: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(right_answers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[85], line 11\u001b[0m, in \u001b[0;36mrun_inference_step1\u001b[0;34m(prompt_t, use_api)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_data)):        \n\u001b[1;32m     10\u001b[0m     prompt_t\u001b[38;5;241m=\u001b[39mtemp_prompt_t\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<INPUT>\u001b[39m\u001b[38;5;124m'\u001b[39m,train_data[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 11\u001b[0m     op\u001b[38;5;241m=\u001b[39mget_answer_llm(prompt_t,use_api)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     13\u001b[0m         not_got\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "Cell \u001b[0;32mIn[78], line 7\u001b[0m, in \u001b[0;36mget_answer_llm\u001b[0;34m(user_prompt, use_api, is_summary, temp, topP)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LLM \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      6\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>[INST] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_llama_api(prompt,is_summary)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m LLM \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     10\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m user_prompt\n",
      "Cell \u001b[0;32mIn[94], line 14\u001b[0m, in \u001b[0;36mget_llama_api\u001b[0;34m(inp, is_summary)\u001b[0m\n\u001b[1;32m     12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_summary\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     output \u001b[38;5;241m=\u001b[39m query({\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: inp,\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m2048\u001b[39m}\n\u001b[1;32m     17\u001b[0m     })\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     output \u001b[38;5;241m=\u001b[39m query({\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: inp,\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m4096\u001b[39m}\n\u001b[1;32m     22\u001b[0m     })\n",
      "Cell \u001b[0;32mIn[94], line 11\u001b[0m, in \u001b[0;36mget_llama_api.<locals>.query\u001b[0;34m(payload)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquery\u001b[39m(payload):\n\u001b[1;32m     10\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(API_URL, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/requests/models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_iterations_T=10\n",
    "succesive_prompts=[]\n",
    "start=time.time()\n",
    "file_path = \"results_llama_random_sampling_2811.txt\"\n",
    "\n",
    "\n",
    "for t in range(num_iterations_T):\n",
    "    # Step 1: Inference\n",
    "    s1=time.time()\n",
    "    wrong_answers,right_answers,pos,neg,ng=run_inference_step1(prompt_t)\n",
    "    print(f\"Wrong Ans: {len(wrong_answers)}\")\n",
    "    print(f\"Correct Ans: {len(right_answers)}\")\n",
    "    print(f\"Not got: {len(ng)}\")\n",
    "    print('inference finished in',time.time()-s1)\n",
    "    \n",
    "    # Step 2: Sampling\n",
    "#     samples = random_balanced_sampling(wrong_answers,right_answers,pos,neg)\n",
    "    samples = random_sampling(wrong_answers)\n",
    "    print(samples)\n",
    "    \n",
    "    # Step 3: Generate Hints\n",
    "    s2=time.time()\n",
    "    hints=get_hints_residual_step2(samples)\n",
    "    print('Hints finished in',time.time()-s2)\n",
    "    \n",
    "    # Step 4: Summarize Hints\n",
    "    s3=time.time()\n",
    "    summarised_prompt=get_summarise_step4(hints)\n",
    "    print('summarisation finished in',time.time()-s3)\n",
    "    \n",
    "    \n",
    "    final_prompt=task_description+\"\\n\"+'Some useful hints are: '+summarised_prompt+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "    succesive_prompts.append(final_prompt)\n",
    "    prompt_t=final_prompt\n",
    "    \n",
    "    print(time.time()-start)\n",
    "    with open(file_path, \"a\") as file:\n",
    "    # Write the string to the file\n",
    "        file.write(f\"Iteration {str(t)} \\n {prompt_t} \\n\\n Wrong Ans: {len(wrong_answers)} \\n Correct Ans: {len(right_answers)} \\n Not got: {len(ng)} \\n Total time : {time.time()-start} \\n\\n\\n \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 69800.12289500237\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_out(final_prompt_totest,use_api=True):\n",
    "    wrong_ans_indices=[]\n",
    "    not_got=[]\n",
    "    task_type = 'implicatures'\n",
    "    for i in range(len(test_data)):\n",
    "        prompt_t=final_prompt_totest.replace('<INPUT>',test_data[i]['input'])\n",
    "        op=get_answer_llm(prompt_t,use_api)\n",
    "        if op=='':\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        output_str=op\n",
    "        ans=extract_answer(len(prompt_t) if LLM==\"llama\" else 0,output_str)\n",
    "        if ans==-1:\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        ground_truth=1\n",
    "      \n",
    "        if task_type=='entailment':\n",
    "            if test_data[i]['target_scores']['non-entailment']==1:\n",
    "                ground_truth=0\n",
    "        elif task_type=='hyperbaton':\n",
    "            if test_data[i]['target_scores']['b']==1:\n",
    "                ground_truth=0\n",
    "        elif task_type=='implicatures':\n",
    "            if test_data[i]['target_scores'][\"no\"]==1.0:\n",
    "                ground_truth=0\n",
    "        if (ans!=ground_truth):\n",
    "            wrong_ans_indices.append(i)\n",
    "    return wrong_ans_indices,not_got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Determine whether one sentence entails the next\\nSome useful hints are: The entailment or non-entailment between a premise and a hypothesis depends on the information shared between them. If the hypothesis is a simplified version of the premise, containing all the information from the premise but without additional details, then the premise entails the hypothesis. However, if the premise only suggests or suspects something about another person's knowledge or belief, it does not confirm that the person actually holds that knowledge or belief. Therefore, the premise does not necessarily entail the hypothesis in such cases.\\nGiven input: <INPUT>\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "succesive_prompts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Predict whether Speaker 2's answer to Speaker 1 counts as a yes or as a no\\nGiven input: <INPUT>\\nDoes Speaker 2's answer mean yes or no?\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "prompt_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper_prompt = \"Determine whether one sentence entails the next\\nSome useful hints are:\\n-Entailment occurs when the hypothesis is a logical consequence of the premise, or when the premise guarantees the truth of the hypothesis, regardless of the level of specificity or simplification of the terms Involved.\\n-Non-entailment occurs when the premise does iet guarantee the truth of the hypothesis, or when there is a possibility that the hypothesis is false or unknown, especially when the premise involves beliefs or thoughts of other people.\\nGiven input: <INPUT>\\nIdentify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\nPut your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_prompt = \"Give a summary of the reasons for the example output, and do not give a reason particular to the respective example. Also do not mention the number of examples nor give any reference to entities in examples in the summary directly or indirectly. Be as general as possible. The summarised reasons are: [/INST]  The reasons for the expected outputs in the given examples can be summarized as follows: \\n * The answer may not directly address the question but still imply a positive response, downplaying the action or trying to shift focus away from it. \\n * The answer may indicate a problem or issue that needs to be addressed, which implies a willingness to engage in a conversation or take action. \\n * The answer may provide information that suggests the person knows how to get to the destination, even if it's not a direct answer to the question. \\n\\n In general, the reasons for the expected outputs suggest that the model should look beyond the literal meaning of the answer and consider the implied meaning, context, and potential idioms or phrases that may be used to convey a different message. \\n Given input: <INPUT> \\n Does Speaker 2's answer mean yes or no? \\n Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_ans_indice_test,not_got_test=test_out(last_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_ans_indice_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_got_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "pos_datapoints = [i for i in range(len(test_data)) if test_data[i]['target_scores']['yes'] == 1.0]\n",
    "neg_datapoints = [i for i in range(len(test_data)) if test_data[i]['target_scores']['no'] == 1.0]\n",
    "\n",
    "print(len(pos_datapoints))\n",
    "print(len(neg_datapoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
