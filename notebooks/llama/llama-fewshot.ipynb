{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_DATASETS_CACHE=\"/scratch/pt2295/cache/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/pt2295/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"/scratch/pt2295/LlvmProject/automatic_prompt_engineer/data/bigbench-ii/epistemic_reasoning/task.json\"\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    \n",
    "    data_orig = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_orig['examples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_orig['examples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "\n",
    "# Define the proportions for train, test, and validation sets\n",
    "total_samples = len(data)\n",
    "train_ratio = 0.7\n",
    "test_ratio = 0.15\n",
    "validation_ratio = 0.15\n",
    "\n",
    "# Calculate the sizes of each set\n",
    "train_size = int(total_samples * train_ratio)\n",
    "test_size = int(total_samples * test_ratio)\n",
    "validation_size = int(total_samples * validation_ratio)\n",
    "\n",
    "# Split the data into sets\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:train_size + test_size]\n",
    "validation_data = data[train_size + test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save train_data to train.json\n",
    "with open('data/train_hyper.json', 'w') as train_file:\n",
    "    json.dump(train_data, train_file, indent=4)\n",
    "\n",
    "# Save test_data to test.json\n",
    "with open('data/test_hyper.json', 'w') as test_file:\n",
    "    json.dump(test_data, test_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/val_hyper.json', 'w') as val_file:\n",
    "    json.dump(validation_data, val_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read data from train.json\n",
    "with open('data/train_entail.json', 'r') as train_file:\n",
    "    train_data = json.load(train_file)\n",
    "\n",
    "# Read data from test.json\n",
    "with open('data/test_entail.json', 'r') as test_file:\n",
    "    test_data = json.load(test_file)\n",
    "with open('data/val_entail.json', 'r') as val_file:\n",
    "    validation_data=json.load(val_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCOMMENT THIS TO LOAD MODEL\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "custom_cache_dir = \"/scratch/pt2295/cache/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=custom_cache_dir,use_fast=True)\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True,cache_dir=custom_cache_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description=data_orig['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type='entailment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix=data_orig['task_prefix']\n",
    "if task_type=='hyperbaton':\n",
    "    task_prefix=task_prefix.strip()\n",
    "    task_prefix+=\" Choose only from the following options: 'a' or 'b'.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_answer(length,line,task_type='entailment'):\n",
    "    line=line[length:]\n",
    "    pattern = r'<Ans>(.*?)</Ans>|Ans:\\s*([\\w-]+)'\n",
    "    matches = re.findall(pattern, line)\n",
    "    if len(matches)==0:#right now if not match return\n",
    "        return -1\n",
    "    \n",
    "    matches=matches[0][0]\n",
    "   \n",
    "    if task_type=='entailment':\n",
    "        if matches =='entailment' or matches=='Entailment' :#or matches not in ['non-entailment','Non-entailment','Non-Entailment']:\n",
    "            return 1\n",
    "    elif task_type=='hyperbaton':\n",
    "        if matches[0][0]=='a' or matches[0][0]=='A':\n",
    "            return 1\n",
    "    return 0\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_answer(0,\"<Ans>b</Ans>\",task_type='hyperbaton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-70b-chat-hf\"\n",
    "headers = {\"Authorization\": \"**********\"}#hiding API key for security\n",
    "\n",
    "def get_api(inp,type_prompt=\"infer\"):\n",
    "    def query(payload):\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        return response.json()\n",
    "    if type_prompt==\"infer\":\n",
    "        output = query({\n",
    "            \"inputs\": inp,\n",
    "            \"parameters\":{\"max_new_tokens\":1000,\"do_sample\":True,\"temperature\":0.75,\"top_p\":0.9,\"use_cache\":True}\n",
    "        })\n",
    "    elif type_prompt==\"summary\":\n",
    "        output = query({\n",
    "            \"inputs\": inp,\n",
    "            \"parameters\":{\"max_new_tokens\":4096,\"do_sample\":True,\"temperature\":0.75,\"top_p\":0.9}\n",
    "        })\n",
    "        \n",
    "    else:\n",
    "        output = query({\n",
    "            \"inputs\": inp,\n",
    "            \"parameters\":{\"max_new_tokens\":1000,\"do_sample\":True,\"temperature\":0.75,\"top_p\":0.9}\n",
    "        })\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE THERE IS THE PROVISION OF USING LOCAL LLAMA MODEL OR INFERENCE API FROM HUGGINGFACE\n",
    "def get_answer_llm(user_prompt,use_api=True,type_prompt='infer'):\n",
    "\n",
    "    sys_prompt=\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "    prompt=f\"You are a helpful assistant. <s>[INST] {user_prompt} [/INST]\"\n",
    "    \n",
    "    if use_api==False:\n",
    "   \n",
    "        model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        output = model.generate(**model_inputs)\n",
    "\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    else:\n",
    "\n",
    "        try:\n",
    "            return get_api(prompt,type_prompt)[0]['generated_text']\n",
    "        except:\n",
    "            return \"\"\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: RUN INFERENCE FROM LLM TO GET CORRECT AND INCORRECT ANSWERS\n",
    "def run_inference_step1(prompt_t,use_api=True,task_type='entailment'):\n",
    "    wrong_ans_indices={1:[],0:[]}\n",
    "    not_got=[]\n",
    "    temp_prompt_t=prompt_t\n",
    "    pos=[]\n",
    "    neg=[]\n",
    "    for i in range(len(train_data)):\n",
    "        \n",
    "        prompt_t=temp_prompt_t.replace('<INPUT>',train_data[i]['input'])\n",
    "        op=get_answer_llm(prompt_t,use_api,type_prompt=\"infer\")\n",
    "        if op=='':\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        output_str=op\n",
    "  \n",
    "        ans=extract_answer(len(prompt_t),output_str,task_type)\n",
    "        if ans==-1:\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        ground_truth=1\n",
    "        \n",
    "        if task_type=='entailment':\n",
    "            if train_data[i]['target_scores']['non-entailment']==1:\n",
    "                ground_truth=0\n",
    "        elif task_type=='hyperbaton':\n",
    "            if train_data[i]['target_scores']['b']==1:\n",
    "                ground_truth=0\n",
    "\n",
    "        if (ans!=ground_truth):\n",
    "            \n",
    "            wrong_ans_indices[ground_truth].append(i)\n",
    "        if ground_truth==1:\n",
    "            pos.append(i)\n",
    "        else:\n",
    "            neg.append(i)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "    return wrong_ans_indices,not_got,pos,neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hint(length,line):\n",
    "    line=line[length:]\n",
    "    pattern = r'<hint>(.*?)</hint>'\n",
    "    matches = re.findall(pattern, line)\n",
    "    if len(matches)==0:\n",
    "        return \"\"\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(wrong_answers,k=20):\n",
    "    \n",
    "    k=min(k,math.ceil(len(wrong_answers)))\n",
    "        \n",
    "    print(\"k values is\", k)\n",
    "    \n",
    "    selected=random.sample(wrong_answers, k)\n",
    "    return selected\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: RUN HINTS GENERATION FOR THE SAMPLES\n",
    "def get_hints_residual_step2(wrong_answers,pos,neg,use_api=True):\n",
    "    hints_wrong_ans={}\n",
    "    \n",
    "    selected_pos_answers=random_sampling(list(wrong_answers[1]),3)\n",
    "    if len(selected_pos_answers)==0:\n",
    "        selected_pos_answers=random_sampling(list(pos),3)\n",
    "        \n",
    "    selected_neg_answers=random_sampling(list(wrong_answers[0]),3)\n",
    "    if len(selected_neg_answers)==0:\n",
    "        selected_neg_answers=random_sampling(list(neg),3)\n",
    "    selected_answers=selected_pos_answers+selected_neg_answers\n",
    "    print(\"len of selected answers in hints\",len(selected_answers))\n",
    "    \n",
    "    for idx in selected_answers:\n",
    "        if task_type=='entailment':\n",
    "            if train_data[idx]['target_scores']['entailment']==1:\n",
    "                ans='entailment'\n",
    "            else:\n",
    "                ans='non-entailment'\n",
    "        elif task_type=='hyperbaton':\n",
    "            if train_data[idx]['target_scores']['a']==1:\n",
    "                ans='a'\n",
    "            else:\n",
    "                ans='b'\n",
    "        prompt_h='Given following task:'+task_description+'.'+'Given input: '+train_data[idx]['input']+'.'+'And its expected output: '+ans+'\\n'+'List the reason or hint why its with this expected output within tag <hint> and </hint>. The hint or explaination should be necassarily between the tags.'\n",
    "       \n",
    "        op=get_answer_llm(prompt_h,use_api,type_prompt=\"hint\")\n",
    "        \n",
    "\n",
    "        hint=extract_hint(len(prompt_h),op)\n",
    "        \n",
    "        if idx%100==0:\n",
    "            print(\"raw output\",op,len(op))\n",
    "            print(\"****************\")\n",
    "            print(\"hint is\",hint)\n",
    "\n",
    "       \n",
    "        if hint!='':\n",
    "            hints_wrong_ans[idx]=hint\n",
    "   \n",
    "       \n",
    "        \n",
    "    return hints_wrong_ans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 3 is sampling and is already done and step4 is summarisation of hints\n",
    "def get_summarise_step4(hints_wrong,use_api=True):\n",
    "\n",
    "    prompt_s=\"This is a task to \"+task_description+\". We have some expected input and output pairs and have asked labeler to give reason or hint for each expected output. Given following data each contains input, output and reason for the expected output, summarize a general reason for all these cases:\"+'\\n'\n",
    "    temp_str=''\n",
    "    ctr=1\n",
    "\n",
    "    for idx in list(hints_wrong.keys()):\n",
    "        \n",
    "        if len(temp_str)>=4095:\n",
    "            print(\"the length of temp str is\",len(temp_str))\n",
    "            break\n",
    "        temp=''\n",
    "        if task_type=='entailment':\n",
    "            if train_data[idx]['target_scores']['entailment']==1:\n",
    "                ans='entailment'\n",
    "            else:\n",
    "                ans='non-entailment'\n",
    "        elif task_type=='hyperbaton':\n",
    "            if train_data[idx]['target_scores']['a']==1:\n",
    "                ans='a'\n",
    "            else:\n",
    "                ans='b'\n",
    "\n",
    "        temp+='Given input: '+train_data[idx]['input']+' '+'And its expected output: '+ans+'. And the reason for the expected output: '+hints_wrong[idx]\n",
    "        temp_str+=temp\n",
    "   \n",
    "        \n",
    "    prompt_s+=temp_str+'\\n'+\"Give a summary of the reasons for the example output, and do not give a reason particular to the respective example. Also do not mention the number of examples nor give any reference to entities in examples in the summary directly or indirectly. Be as general as possible. The summarised reasons are:\"\n",
    "\n",
    "    op=get_answer_llm(prompt_s,use_api,type_prompt=\"summary\")#[l+8+7:]#or extract after [/INSTR]\n",
    "    index = op.find(\"[/INST\")\n",
    "    \n",
    "    extracted_text = op[index + len(\"[/INST]\"):]\n",
    "    op=extracted_text\n",
    "    print(\"summarised op isyyyyy\",op)\n",
    "    return op.lstrip()\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have to replicate all these steps for T iterations, just a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_iterations_T=10\n",
    "succesive_prompts=[]\n",
    "start=time.time()\n",
    "file_path = \"data/hyper_llama_check_summary.txt\"\n",
    "\n",
    "prompt_t=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "for t in range(num_iterations_T):\n",
    "    s1=time.time()\n",
    "    wrong_answers,ng,pos,neg=run_inference_step1(prompt_t,True,task_type)\n",
    "    time.sleep(5)\n",
    "    print(\"number of wrong_ans are\",len(wrong_answers[1]),len(wrong_answers[0]),len(ng))\n",
    "    print('inference finished in',time.time()-s1)\n",
    "    s2=time.time()\n",
    "    hints_wrong=get_hints_residual_step2(wrong_answers,pos,neg)\n",
    "    time.sleep(5)\n",
    "    print(\"num of hints\",len(hints_wrong))\n",
    "    print('hints finished in',time.time()-s2)\n",
    "    s3=time.time()\n",
    "    summarised_prompt=get_summarise_step4(hints_wrong)\n",
    "    time.sleep(5)\n",
    "    print('summarisation finished in',time.time()-s3)\n",
    "    final_prompt=task_description+\"\\n\"+'Some useful hints are: '+summarised_prompt+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "    succesive_prompts.append(final_prompt)\n",
    "    prompt_t=final_prompt\n",
    "    print(prompt_t)\n",
    "    print(time.time()-start)\n",
    "    with open(file_path, \"a\") as file:\n",
    "    # Write the string to the file\n",
    "        file.write(prompt_t+\"\\n\"+\"Iteration\"+str(t)+\"\\n\")\n",
    "print(\"Time taken\",time.time()-start)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succesive_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(succesive_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succesive_prompts[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now test basline and improved few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_classes_train():\n",
    "    pos_idx=[]\n",
    "    neg_idx=[]\n",
    "    for i in range(len(train_data)):\n",
    "        if task_type=='entailment':\n",
    "            if train_data[i]['target_scores']['entailment']==1:\n",
    "                pos_idx.append(i)\n",
    "            else:\n",
    "                neg_idx.append(i)\n",
    "        elif task_type=='hyperbaton':\n",
    "            if train_data[i]['target_scores']['a']==1:\n",
    "                pos_idx.append(i)\n",
    "            else:\n",
    "                neg_idx.append(i)\n",
    "                \n",
    "    return pos_idx,neg_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "#here BERT embeddings are created and indexed into dict for fast retrieval and cosine ditance is calcualted based on this dict\n",
    "def create_embedding(sentence, model, tokenizer):\n",
    "    tokens = tokenizer(sentence, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "    embeddings = output.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "\n",
    "def create_embedding_dict(training_data, model, tokenizer):\n",
    "    embedding_dict = {}\n",
    "    for idx, sentence in enumerate(training_data):\n",
    "        embedding = create_embedding(sentence, model, tokenizer)\n",
    "        embedding_dict[idx] = embedding\n",
    "    return embedding_dict\n",
    "\n",
    "def find_top_least_similar(input_sentence, embedding_dict, top_k=3):\n",
    "    input_embedding = create_embedding(input_sentence, model, tokenizer)\n",
    "    similarities = {idx: cosine(input_embedding, emb) for idx, emb in embedding_dict.items()}\n",
    "    top_least_similar_idx = sorted(similarities, key=similarities.get)[:top_k]\n",
    "    return top_least_similar_idx\n",
    "\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "training_data=[]\n",
    "for i in range(len(train_data)):\n",
    "\n",
    "    training_data.append(train_data[i]['input'])\n",
    "\n",
    "embedding_dict = create_embedding_dict(training_data, model, tokenizer)\n",
    "\n",
    "#test out few shot methods whether working or not\n",
    "input_sentence = \"Premise: Prapti is waeing a white dress. Hypothesis: Prapti is getting married.\"\n",
    "most_similar_idx = find_top_least_similar(input_sentence, embedding_dict)\n",
    "\n",
    "print(f\"The most similar index is: {most_similar_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here vanilla=True is for baseline random sampling and vanilla=False is for our proposed method\n",
    "def add_few_shot(prompt,pos_idx,neg_idx,vanilla,input_sentence=None):\n",
    "    index = prompt.find(task_description)\n",
    "    if index != -1:\n",
    "        to_add='. Following are some examples for the task:\\n'\n",
    "        temp_l=[i for i in range(len(train_data))]\n",
    "        if vanilla:\n",
    "            random_fs_pos=random.sample(pos_idx,2)#balanced\n",
    "            random_fs_neg=random.sample(neg_idx,2)\n",
    "            random_fs=random_fs_pos+random_fs_neg\n",
    "        else:\n",
    "            random_fs=find_top_least_similar(input_sentence, embedding_dict)\n",
    "            \n",
    "        for i in random_fs:\n",
    "            if task_type=='entailment':\n",
    "                if train_data[i]['target_scores']['entailment']==1:\n",
    "                    ans='<Ans>entailment</Ans>'\n",
    "                else:\n",
    "                    ans='<Ans>non-entailment</Ans>'\n",
    "            elif task_type=='hyperbaton':\n",
    "                if train_data[i]['target_scores']['a']==1:\n",
    "                    ans='<Ans>a</Ans>'\n",
    "                else:\n",
    "                    ans='<Ans>b</Ans>'\n",
    "            to_add+='Given input: '+train_data[i]['input']+' '+'And its expected output: '+ans+\"\\n\"\n",
    "       \n",
    "        \n",
    "        result_str = prompt[:index + len(task_description)] + to_add + prompt[index + len(task_description):]\n",
    "\n",
    "        return result_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def test_out(successive_prompts,task_type=\"entailment\",use_api=True,check_baseline=False,few_shot=True,vanilla_fs=True):\n",
    "    start=time.time()\n",
    "    if check_baseline==True:\n",
    "        final_prompt_totest=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "    else:\n",
    "        final_prompt_totest=successive_prompts[-1]\n",
    "    pos_idx=[]\n",
    "    neg_idx=[]\n",
    "    if few_shot:\n",
    "        if vanilla_fs==True:\n",
    "            pos_idx,neg_idx=divide_into_classes_train()\n",
    "\n",
    "\n",
    "            final_prompt_totest=add_few_shot(final_prompt_totest,pos_idx,neg_idx,vanilla_fs)\n",
    "        else:\n",
    "            if check_baseline==False:\n",
    "                final_prompt_totest=successive_prompts[-1]\n",
    "            else:\n",
    "                final_prompt_totest=task_description+'\\n'+'Given input: '+'<INPUT>'+'\\n'+task_prefix+'Put your one-word answer choosing from the previously stated two options within tag <Ans> and </Ans>. The one word answer should be necassarily between the tags.'\n",
    "                \n",
    "\n",
    "    ans_accuracy=[]\n",
    "    ans_balanced=[]\n",
    "  \n",
    "        \n",
    "    wrong_ans_indices=[]\n",
    "    not_got=[]\n",
    "\n",
    "    ground_truth_list=[]\n",
    "    predicted_list=[]\n",
    "   \n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "\n",
    "        prompt_t=final_prompt_totest.replace('<INPUT>',test_data[i]['input'])\n",
    "        if vanilla_fs==False:\n",
    "            prompt_tt=add_few_shot(prompt_t,pos_idx,neg_idx,vanilla_fs,test_data[i]['input'])\n",
    "        op=get_answer_llm(prompt_tt,use_api,type_prompt=\"infer\")\n",
    "\n",
    "        if op=='':\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        output_str=op\n",
    "\n",
    "\n",
    "        ans=extract_answer(len(prompt_tt),output_str,task_type)\n",
    "        if ans==-1:\n",
    "            not_got.append(i)\n",
    "            continue\n",
    "        ground_truth=1\n",
    "\n",
    "        if task_type=='entailment':\n",
    "            if test_data[i]['target_scores']['non-entailment']==1:\n",
    "                ground_truth=0\n",
    "        elif task_type=='hyperbaton':\n",
    "            if test_data[i]['target_scores']['b']==1:\n",
    "                ground_truth=0\n",
    "        print(ans,ground_truth)\n",
    "        if (ans!=ground_truth):\n",
    "            wrong_ans_indices.append(i)\n",
    "\n",
    "        ground_truth_list.append(ground_truth)\n",
    "        predicted_list.append(ans)\n",
    "\n",
    "    print(\"Time taken\",time.time()-start)\n",
    "    \n",
    "        \n",
    "            \n",
    "    return wrong_ans_indices,not_got,ground_truth_list,predicted_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_ans_indice_test,not_got_test,y_true,y_pred=test_out(succesive_prompts,check_baseline=True,few_shot=True,vanilla_fs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(wrong_ans_indice_test))\n",
    "print(len(not_got_test))\n",
    "print(balanced_accuracy_score(y_true, y_pred))\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "\n",
    "print('****************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wrong_ans_indice_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_got_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "166+15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
